{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai qdrant-client gradio pandas tiktoken -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jhAYONVPmoR5",
        "outputId": "2694ce89-5e02-4091-a75f-61f10e4d6f3e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.16-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting openai\n",
            "  Downloading openai-1.44.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting qdrant-client\n",
            "  Downloading qdrant_client-1.11.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.43.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.16-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.38 (from langchain)\n",
            "  Downloading langchain_core-0.2.38-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.116-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (1.64.1)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant-client)\n",
            "  Downloading grpcio_tools-1.66.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (2.0.7)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting fastapi<0.113.0 (from gradio)\n",
            "  Downloading fastapi-0.112.4-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.6)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.6.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi<0.113.0->gradio)\n",
            "  Downloading starlette-0.38.5-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting protobuf<6.0dev,>=5.26.1 (from grpcio-tools>=1.41.0->qdrant-client)\n",
            "  Downloading protobuf-5.28.0-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting grpcio>=1.41.0 (from qdrant-client)\n",
            "  Downloading grpcio-1.66.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.41.0->qdrant-client) (71.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.15.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.38->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.0)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading langchain-0.2.16-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.44.0-py3-none-any.whl (367 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.8/367.8 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qdrant_client-1.11.1-py3-none-any.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.4/259.4 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-4.43.0-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.2.16-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading fastapi-0.112.4-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_tools-1.66.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.66.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.38-py3-none-any.whl (396 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.116-py3-none-any.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading ruff-0.6.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.28.0-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.5-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: pydub, websockets, tomlkit, tenacity, semantic-version, ruff, python-multipart, protobuf, portalocker, orjson, mypy-extensions, marshmallow, jsonpointer, jiter, hyperframe, hpack, h11, grpcio, ffmpy, aiofiles, uvicorn, typing-inspect, tiktoken, starlette, pandas, jsonpatch, httpcore, h2, grpcio-tools, httpx, fastapi, dataclasses-json, openai, langsmith, gradio-client, qdrant-client, langchain-core, gradio, langchain-text-splitters, langchain, langchain-community\n",
            "  Attempting uninstall: tomlkit\n",
            "    Found existing installation: tomlkit 0.13.2\n",
            "    Uninstalling tomlkit-0.13.2:\n",
            "      Successfully uninstalled tomlkit-0.13.2\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.64.1\n",
            "    Uninstalling grpcio-1.64.1:\n",
            "      Successfully uninstalled grpcio-1.64.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.1.4\n",
            "    Uninstalling pandas-2.1.4:\n",
            "      Successfully uninstalled pandas-2.1.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.2 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.28.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.25.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.0 which is incompatible.\n",
            "google-cloud-datastore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.0 which is incompatible.\n",
            "google-cloud-firestore 2.16.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.1.4, but you have pandas 2.2.2 which is incompatible.\n",
            "tensorboard 2.17.0 requires protobuf!=4.24.0,<5.0.0,>=3.19.6, but you have protobuf 5.28.0 which is incompatible.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.0 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.28.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 dataclasses-json-0.6.7 fastapi-0.112.4 ffmpy-0.4.0 gradio-4.43.0 gradio-client-1.3.0 grpcio-1.66.1 grpcio-tools-1.66.1 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-1.0.5 httpx-0.27.2 hyperframe-6.0.1 jiter-0.5.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.16 langchain-community-0.2.16 langchain-core-0.2.38 langchain-text-splitters-0.2.4 langsmith-0.1.116 marshmallow-3.22.0 mypy-extensions-1.0.0 openai-1.44.0 orjson-3.10.7 pandas-2.2.2 portalocker-2.10.1 protobuf-5.28.0 pydub-0.25.1 python-multipart-0.0.9 qdrant-client-1.11.1 ruff-0.6.4 semantic-version-2.10.0 starlette-0.38.5 tenacity-8.5.0 tiktoken-0.7.0 tomlkit-0.12.0 typing-inspect-0.9.0 uvicorn-0.30.6 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "openai_api_key=userdata.get('openai_api_key')"
      ],
      "metadata": {
        "id": "_hUcMQoLm85a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iWFYdHj3mYeB"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Qdrant\n",
        "from langchain.chains import VectorDBQA\n",
        "from langchain.llms import OpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qdrant_url=userdata.get('Qdrant')\n",
        "qdrant_api_key=userdata.get('qdrant_api_key')\n",
        "openai_api_key=userdata.get('openai_api_key')\n",
        "# groq_api_key=userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "5Xr1Wg4kJo2_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "\n",
        "\n",
        "#csv loader\n",
        "loader = CSVLoader(file_path='dataa.csv')\n",
        "data=loader.load()\n",
        "\n",
        "#split the documnts\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(data)\n",
        "\n",
        "\n",
        "#embeding\n",
        "embeding=OpenAIEmbeddings(openai_api_key=openai_api_key, model=\"text-embedding-3-small\")\n",
        "\n",
        "\n",
        "#import quantization\n",
        "\n",
        "from langchain.vectorstores import Qdrant\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "from langchain.vectorstores import Qdrant\n",
        "\n",
        "#using qudadrant vector database\n",
        "from qdrant_client import QdrantClient, models\n",
        "qdrant = Qdrant.from_documents(\n",
        "            texts,\n",
        "            embeding,\n",
        "            url=qdrant_url,\n",
        "            prefer_grpc=True,\n",
        "            api_key=qdrant_api_key,\n",
        "            collection_name=\"llm_app\",\n",
        "            quantization_config=models.BinaryQuantization(\n",
        "                binary=models.BinaryQuantizationConfig(\n",
        "                    always_ram=True,\n",
        "                )\n",
        "            )\n",
        ")\n",
        "\n",
        "\n",
        "#qdrant client\n",
        "qdrant_client = QdrantClient(\n",
        "    url=qdrant_url,\n",
        "    prefer_grpc=True,\n",
        "    api_key=qdrant_api_key,\n",
        ")\n",
        "\n",
        "from re import search\n",
        "#retriver\n",
        "retriver=qdrant.as_retriever( search_type=\"similarity\", search_kwargs={\"k\":2})\n",
        "\n",
        "\n",
        "#search query\n",
        "query=\"show me a best darmatology doctor in peshawar  \"\n",
        "docs=retriver.get_relevant_documents(query)\n",
        "\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "        # Your Role\n",
        "        You are a highly skilled AI specialized in healthcare and medical information retrieval. Your expertise lies in understanding the medical needs of patients and accurately matching them with the most suitable healthcare professionals, including but not limited to surgeons, dentists, dermatologists, cardiologists, neurologists, etc., based on the user's query and the provided context.\n",
        "\n",
        "        # Instruction\n",
        "        Your task is to answer the question using the following pieces of retrieved context delimited by XML tags.\n",
        "\n",
        "        <retrieved context>\n",
        "        Retrieved Context:\n",
        "        {context}\n",
        "        </retrieved context>\n",
        "\n",
        "        # Constraint\n",
        "        1. Carefully analyze the user's question:\n",
        "        User's question:\\n{question}\\n\n",
        "        Your goal is to understand the user's needs and match them with the most relevant healthcare professional(s) from the provided context.\n",
        "        - Reflect on why the question was asked, and deliver an appropriate response based on the context you understand.\n",
        "        2. Select the most relevant information (the key details directly related to the question) from the retrieved context and use it to formulate an answer.\n",
        "        3. Generate a comprehensive, logical, and medically accurate answer. When generating the answer, include the following details about the healthcare professional:\n",
        "            • Name of the Professional\n",
        "            • City\n",
        "            • Specialization (e.g., Surgeon, Dentist, Cardiologist, etc.)\n",
        "            • Qualification (e.g., MBBS, FCPS, etc.)\n",
        "            • Years of Experience\n",
        "            • Patient Satisfaction Rate (if available)\n",
        "            • Average Time Spent with Patients (if available)\n",
        "            • Wait Time (if available)\n",
        "            • Hospital/Clinic Address\n",
        "            • Consultation Fee\n",
        "            • Profile Link (if available)\n",
        "        4. If the retrieved context does not contain enough relevant information, or if the documents are irrelevant, respond with 'I can't find the answer to that question in the material I have'.\n",
        "        5. Provide a complete answer to the user. Do not limit the information if there is more useful data available in the retrieved context.\n",
        "        6. At the end of the response, do not include any unnecessary metadata (such as Source, Row, or _id). Only focus on the healthcare professional's information relevant to the user's query.\n",
        "\n",
        "        # Question:\n",
        "        {question}\"\"\",\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "\n",
        "#import ChatOpenAI\n",
        "# llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
        "llm = ChatOpenAI(temperature=0.5, model='gpt-4o', openai_api_key=openai_api_key)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "        formatted_docs = []\n",
        "        for doc in docs:\n",
        "            # Format the metadata into a string\n",
        "            metadata_str = ', '.join(f\"{key}: {value}\" for key, value in doc.metadata.items())\n",
        "\n",
        "            # Combine page content with its metadata\n",
        "            doc_str = f\"{doc.page_content}\\nMetadata: {metadata_str}\"\n",
        "\n",
        "            # Append to the list of formatted documents\n",
        "            formatted_docs.append(doc_str)\n",
        "\n",
        "        # Join all formatted documents with double newlines\n",
        "        return \"\\n\\n\".join(formatted_docs)\n",
        "\n",
        "#import strw\n",
        "\n",
        "        from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "rag_chain = (\n",
        "            {\"context\":  retriver| format_docs, \"question\": RunnablePassthrough()}\n",
        "            | prompt\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "import openai\n",
        "import os\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "def reg(message, history):\n",
        "    history_langchain_format = []\n",
        "    for human, ai in history:\n",
        "        history_langchain_format.append(HumanMessage(content=human))\n",
        "        history_langchain_format.append(AIMessage(content=ai))\n",
        "    history_langchain_format.append(HumanMessage(content=message))\n",
        "    gpt_response = llm(history_langchain_format)\n",
        "    return rag_chain.invoke(message)\n",
        "# Gradio ChatInterface\n",
        "demo = gr.ChatInterface(\n",
        "    fn=reg,\n",
        "    title=\"Doctors Appointments Assistant\",\n",
        "    theme=\"soft\",\n",
        ")\n",
        "\n",
        "demo.launch(show_api=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "IWqJBCy3YM4j",
        "outputId": "9a6a3822-86c6-4485-a7e1-20fb734c8ef4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://9ff9f70472f890835d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9ff9f70472f890835d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#csv loader\n",
        "loader = CSVLoader(file_path='data.csv')\n",
        "data=loader.load()\n"
      ],
      "metadata": {
        "id": "Hp4L-N4PJWmk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "g-Ry7-XXMmEM",
        "outputId": "183249cd-20f9-4aac-f467-822e770251c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'data.csv', 'row': 0}, page_content='Doctor Name: Prof. Dr. Syed Shamsuddin\\nCity: QUETTA\\nSpecialization: Dermatologist\\nDoctor Qualification: MBBS, MCPS, FCPS\\nExperience(Years): 30\\nTotal_Reviews: 827\\nPatient Satisfaction Rate(%age): 98\\nAvg Time to Patients(mins): 14\\nWait Time(mins): 13\\nHospital Address: National Hospital, Near Quetta Laboratory Prince Road, Quetta\\nDoctors Link: https://www.marham.pk/doctors/quetta/dermatologist/prof-dr-syed-shamsuddin#reviews-scroll\\nFee(PKR): 5000'),\n",
              " Document(metadata={'source': 'data.csv', 'row': 1}, page_content='Doctor Name: Dr. Jaffar Ali\\nCity: QUETTA\\nSpecialization: Dermatologist\\nDoctor Qualification: MBBS, MCPS (Dermatology), FDV (Austria)\\nExperience(Years): 32\\nTotal_Reviews: 1250\\nPatient Satisfaction Rate(%age): 97\\nAvg Time to Patients(mins): 16\\nWait Time(mins): 13\\nHospital Address: Quetta Hospital, Quetta City, Quetta\\nDoctors Link: https://www.marham.pk/doctors/quetta/dermatologist/dr-jaffar-ali#reviews-scroll\\nFee(PKR): 3000')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbXom45zMuDU",
        "outputId": "f20131cf-509e-4a1b-8be5-ddee8e973d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2652"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split the documnts\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "py9IWOTlNGEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9F8fP_3NZb6",
        "outputId": "55d80243-087c-4c3d-8a7a-c5e15c970efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2652"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#embeding\n",
        "embeding=OpenAIEmbeddings(openai_api_key=openai_api_key, model=\"text-embedding-3-small\")\n"
      ],
      "metadata": {
        "id": "jV5iGQ_QNfko",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf4abbbc-988e-4d89-a41f-f2776a63649e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-a3672020b1a9>:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  embeding=OpenAIEmbeddings(openai_api_key=openai_api_key, model=\"text-embedding-3-small\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import quantization\n",
        "\n",
        "from langchain.vectorstores import Qdrant\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "from langchain.vectorstores import Qdrant\n",
        "\n",
        "#using qudadrant vector database\n",
        "from qdrant_client import QdrantClient, models\n",
        "qdrant = Qdrant.from_documents(\n",
        "            texts,\n",
        "            embeding,\n",
        "            url=qdrant_url,\n",
        "            prefer_grpc=True,\n",
        "            api_key=qdrant_api_key,\n",
        "            collection_name=\"llm_app\",\n",
        "            quantization_config=models.BinaryQuantization(\n",
        "                binary=models.BinaryQuantizationConfig(\n",
        "                    always_ram=True,\n",
        "                )\n",
        "            )\n",
        ")"
      ],
      "metadata": {
        "id": "yvLT0U81OwzC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#qdrant client\n",
        "qdrant_client = QdrantClient(\n",
        "    url=qdrant_url,\n",
        "    prefer_grpc=True,\n",
        "    api_key=qdrant_api_key,\n",
        ")"
      ],
      "metadata": {
        "id": "Z2RGAACsSNP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import search\n",
        "#retriver\n",
        "retriver=qdrant.as_retriever( search_type=\"similarity\", search_kwargs={\"k\":2})"
      ],
      "metadata": {
        "id": "DJpUuGYnwIpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#search query\n",
        "query=\"show me a best darmatology doctor in peshawar  \"\n",
        "docs=retriver.get_relevant_documents(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8ObxrxFyTOH",
        "outputId": "51c31771-1c78-4aeb-c806-50b459351b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-92f5a93d476f>:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
            "  docs=retriver.get_relevant_documents(query)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#write a code for prety print\n",
        "for i in docs:\n",
        "  print(i.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LNU0sYkywx2",
        "outputId": "86467b20-0c68-4d1c-bbb5-ef77a0743404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doctor Name: Dr. Shahzad Rashid Awan\n",
            "City: PESHAWAR\n",
            "Specialization: Dermatologist\n",
            "Doctor Qualification: MBBS, MCPS (Dermatology)*\n",
            "Experience(Years): 12\n",
            "Total_Reviews: 14\n",
            "Patient Satisfaction Rate(%age): 93\n",
            "Avg Time to Patients(mins): 13\n",
            "Wait Time(mins): 10\n",
            "Hospital Address: Rahim Medical Center And Hospital, Hasht Nagri, Peshawar\n",
            "Doctors Link: https://www.marham.pk/doctors/peshawar/dermatologist/dr-shahzad-rashid-awan#reviews-scroll\n",
            "Fee(PKR): 1000\n",
            "Doctor Name: Dr. Fahad Faizullah\n",
            "City: PESHAWAR\n",
            "Specialization: Dermatologist\n",
            "Doctor Qualification: MBBS, FCPS (Dermatology)\n",
            "Experience(Years): 8\n",
            "Total_Reviews: 3\n",
            "Patient Satisfaction Rate(%age): 100\n",
            "Avg Time to Patients(mins): 17\n",
            "Wait Time(mins): 7\n",
            "Hospital Address: Aesthetica Plastika, Khyber Pakhtunkhwa, Peshawar\n",
            "Doctors Link: https://www.marham.pk/doctors/peshawar/dermatologist/dr-fahad-faizullah#reviews-scroll\n",
            "Fee(PKR): 1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].metadata.items()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opKkU0xU4GGc",
        "outputId": "f86398eb-310c-4cd1-8f20-95219d8fc0ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('source', 'data.csv'), ('row', 1807), ('_id', 'cd86a8bf-cb80-4f70-a723-189eef020607'), ('_collection_name', 'llm_app_02')])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "        # Your Role\n",
        "        You are a highly skilled AI specialized in healthcare and medical information retrieval. Your expertise lies in understanding the medical needs of patients and accurately matching them with the most suitable healthcare professionals based on the given context.\n",
        "\n",
        "        # Instruction\n",
        "        Your task is to answer the question using the following pieces of retrieved context delimited by XML tags.\n",
        "\n",
        "        <retrieved context>\n",
        "        Retrieved Context:\n",
        "        {context}\n",
        "        </retrieved context>\n",
        "\n",
        "        # Constraint\n",
        "        1. Carefully consider the user's question:\n",
        "        User's question:\\n{question}\\n\n",
        "        Analyze the intent behind the question, particularly in relation to the medical context, and provide a precise and helpful answer.\n",
        "        - Reflect on why the question was asked and provide an appropriate response based on the context you understand.\n",
        "        2. Select the most relevant information (the key details directly related to the question) from the retrieved context and use it to formulate an answer.\n",
        "        3. Generate a concise, logical, and medically accurate answer. When generating the answer, include the following details about the doctor in a bulleted format:\n",
        "            • Doctor Name: Dr. Shahzad Rashid Awan\n",
        "            • City: Peshawar\n",
        "            • Specialization: Dermatologist\n",
        "            • Qualification: MBBS, MCPS (Dermatology)\n",
        "            • Experience: 12 years\n",
        "            • Patient Satisfaction Rate: 93%\n",
        "            • Avg Time to Patients: 13 mins\n",
        "            • Wait Time: 10 mins\n",
        "            • Hospital Address: Rahim Medical Center And Hospital, Hasht Nagri, Peshawar\n",
        "            • Fee: PKR 1000\n",
        "            • Profile Link: https://www.marham.pk/doctors/peshawar/dermatologist/dr-shahzad-rashid-awan#reviews-scroll\n",
        "        4. If the retrieved context does not contain information relevant to the question, or if the documents are irrelevant, respond with 'I can't find the answer to that question in the material I have'.\n",
        "        5. Limit the answer to five sentences maximum. Ensure the answer is concise, logical, and medically appropriate.\n",
        "        6. At the end of the response, provide the doctor's profile metadata as shown in the relevant documents, ensuring all bullet points are clearly mentioned.\n",
        "\n",
        "        # Question:\n",
        "        {question}\"\"\",\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "b-XeMobLy39G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #import conversation\n",
        "# from langchain.memory import ConversationBufferMemory\n",
        "# memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
      ],
      "metadata": {
        "id": "wOwRkASwUuBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain-openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "67_veXQwsCsH",
        "outputId": "abde6c97-221c-461a-9806-99a59fc223e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.1.23-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.35 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.2.38)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.43.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain-openai) (0.1.111)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain-openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain-openai) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain-openai) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.66.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.35->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.35->langchain-openai) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.35->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.35->langchain-openai) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.0.7)\n",
            "Downloading langchain_openai-0.1.23-py3-none-any.whl (51 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-openai\n",
            "Successfully installed langchain-openai-0.1.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import ChatOpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "hXSSLUm41jSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "        formatted_docs = []\n",
        "        for doc in docs:\n",
        "            # Format the metadata into a string\n",
        "            metadata_str = ', '.join(f\"{key}: {value}\" for key, value in doc.metadata.items())\n",
        "\n",
        "            # Combine page content with its metadata\n",
        "            doc_str = f\"{doc.page_content}\\nMetadata: {metadata_str}\"\n",
        "\n",
        "            # Append to the list of formatted documents\n",
        "            formatted_docs.append(doc_str)\n",
        "\n",
        "        # Join all formatted documents with double newlines\n",
        "        return \"\\n\\n\".join(formatted_docs)"
      ],
      "metadata": {
        "id": "q8IkSJXb50AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "rag_chain = (\n",
        "            {\"context\":  retriver| format_docs, \"question\": RunnablePassthrough()}\n",
        "            | prompt\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "        )"
      ],
      "metadata": {
        "id": "s68QaKPs3A6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke(\"show me a best darmatology doctor in lahore  \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "rJuITBS86Rob",
        "outputId": "32d63aa7-a0b3-4926-be12-e2fea0b91a2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the information retrieved, here is a highly recommended dermatologist in Lahore:\\n\\n- **Doctor Name:** Dr. Muhammad Amjad\\n- **City:** Lahore\\n- **Specialization:** Dermatologist\\n- **Qualification:** MBBS, MCPS (Dermatology), FCPS (Dermatology)\\n- **Experience:** 35 years\\n- **Patient Satisfaction Rate:** 99%\\n- **Avg Time to Patients:** 13 mins\\n- **Wait Time:** 4 mins\\n- **Hospital Address:** Zarar Shaheed Trust Hospital, Barki, Lahore\\n- **Fee:** PKR 1000\\n- **Profile Link:** [Dr. Muhammad Amjad](https://www.marham.pk/doctors/lahore/dermatologist/dr-muhammad-amjad#reviews-scroll)\\n\\n**Profile Metadata:**\\n- **Source:** data.csv\\n- **Row:** 1497\\n- **_id:** 4e2695f8-f1df-4bac-bdd3-aec0ae966eed\\n- **_collection_name:** llm_app'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "# import gradio as gr\n",
        "\n",
        "# # Gradio Interface\n",
        "# def search_doctor(input_text):\n",
        "#     return rag_chain.invoke(input_text)\n",
        "\n",
        "# # Create the Gradio interface\n",
        "# iface = gr.Interface(\n",
        "#     fn=search_doctor,\n",
        "#     inputs=gr.Textbox(lines=1, label=\"Ask a medical question\"),\n",
        "#     outputs=gr.Textbox(label=\"Answer\"),\n",
        "#     title=\"Medical Assistant\",\n",
        "#     description=\"Find the best doctors based on your medical needs.\",\n",
        "#     allow_flagging=\"never\",\n",
        "#     theme=\"default\",\n",
        "#     css=\".gradio-container {border-radius: 10px; padding: 10px; background-color: #f9f9f9;} .gr-button {visibility: hidden;}\"\n",
        "# )\n",
        "\n",
        "# # Launch the interface without the Gradio logo\n",
        "# iface.launch(show_api=False)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sL9TqXtC-rbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gradio as gr\n",
        "\n",
        "# # Example RAG model invocation function (replace with your actual function)\n",
        "# def rag_model_query(query):\n",
        "#     # Replace with actual RAG model invocation\n",
        "#     return rag_chain.invoke(query)\n",
        "\n",
        "# # Define the Gradio function to handle both echo and RAG queries\n",
        "# def handle_message(message, history):\n",
        "#     # Check if the message contains a keyword to trigger RAG model\n",
        "#     if \"doctor\" in message[\"text\"].lower():\n",
        "#         response = rag_model_query(message[\"text\"])\n",
        "#     else:\n",
        "#         response = message[\"text\"]\n",
        "#     return response\n",
        "\n",
        "# # Create the Gradio interface\n",
        "# demo = gr.ChatInterface(\n",
        "#     fn=handle_message,\n",
        "#     title=\"Medical Assistant\",\n",
        "#     multimodal=True,\n",
        "# )\n",
        "\n",
        "# demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "qDoLEEw6lo9O",
        "outputId": "9b9c3e49-95c8-4aa6-8853-ab3110063c26",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://902c09d5057fe29f2c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://902c09d5057fe29f2c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "import openai\n",
        "import os\n",
        "import gradio as gr\n",
        "# os.environ[\"OPENAI_API_KEY\"] = openai_api_key  # Replace with your key\n",
        "\n",
        "llm = ChatOpenAI(temperature=1.0, model='gpt-4o', openai_api_key=openai_api_key)\n",
        "# llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key, memory=memory)\n",
        "\n",
        "def reg(message, history):\n",
        "    history_langchain_format = []\n",
        "    for human, ai in history:\n",
        "        history_langchain_format.append(HumanMessage(content=human))\n",
        "        history_langchain_format.append(AIMessage(content=ai))\n",
        "    history_langchain_format.append(HumanMessage(content=message))\n",
        "    gpt_response = llm(history_langchain_format)\n",
        "    return rag_chain.invoke(message)\n",
        "# # Gradio ChatInterface\n",
        "# demo = gr.ChatInterface(\n",
        "#     fn=reg,\n",
        "#     title=\"Medical Assistant\",\n",
        "#     # theme=\"soft\",\n",
        "# )\n",
        "\n",
        "# # Apply custom CSS and launch the interface\n",
        "# demo.launch(show_api=False)\n",
        "gr.ChatInterface(predict).launch()\n"
      ],
      "metadata": {
        "id": "ELkXoXqjnsqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lx-m2HRi4xFf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}